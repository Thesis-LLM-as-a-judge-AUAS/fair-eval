![fig1](./fig/fig1.png)

<h1 align="center">Large Language Models are not Fair Evaluators</h1>

# ðŸ”¥ Introduction


ChatGPT and GPT-4  have recently demonstrated remarkable performance across various tasks, leading to their widespread use as both the human annotators and evaluators.
However, it is not clear how reliable LLMs are as evaluators, as they are known to be sensitive to textual instructions and inputs.
In this paper, we critically examine the LLMs-as-evaluator paradigm:



- We reveal that LLMs exhibit severe positional bias, compromising their fairness as evaluators.


- We develop two simple yet effective strategies, namely Multiple Evidence Calibration (MEC) and Balanced Position Calibration (BPC) to calibrate the positional bias of LLMs.


- We demonstrate the effectiveness of our proposed approach through experimental results, which show closer alignment with human judgments.

![fig3](./fig/fig3.png)



# ðŸš€ Run FairEval 

Run evaluation with MEC and BPC strategies:

```bash
m1=gpt35
m2=vicuna-13b
eval_model=gpt-3.5-turbo-0301 # evaluaotr gpt-4 or gpt-3.5-turbo-0301
bpc=1  # 0/1 whether use the BPC strategy
k=3 # the evidence number of MEC strategy

# edit your openai key in FairEval.py first
python3 FairEval.py \
    -q question.jsonl \
    -a answer/answer_$m1.jsonl answer/answer_$m2.jsonl \
    -o review/review_${m1}_${m2}_${eval_model}_mec${k}_bpc${bpc}.json \
    -m $eval_model \
    --bpc $bpc \
    -k $k 
    
```
We have also included the human judgment results for the responses generated by Vicuna-13b and ChatGPT in the file `review/review_gpt35_vicuna-13b_human.txt`.


# Reference

If you find our work helpful, please kindly cite

```bib
@article{Wang2023LargeLM,
  title={Large Language Models are not Fair Evaluators},
  author={Peiyi Wang and Lei Li and Liang Chen and Dawei Zhu and Binghuai Lin and Yunbo Cao and Qi Liu and Tianyu Liu and Zhifang Sui},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.17926},
}
```

# ðŸš€ Sanity check

## Experiments

To replicate the results of the paper we performed experiments for 4 sets of parameters and 2 LLMs (GPT-4 and GPT-3.5):
- EC (k == 1)
- MEC (k == 3)
- MEC (k == 6)
- MEC + BPC (k == 3)

Since the paper declares that its results were obtained as the average of 100 experiments for 80 questions, the minimum number of queries for GPT-4 and GPT-3.5 would be **256000**.

Due to resource constraints, it was decided to sample a portion of the total population as follows:
- randomly sample 1 question of each category (there is **9 categories**)
- assign 9 questions to the set of parameters

Thus, we covered **11.25%** of the whole population while maintaining a balance in the categories of questions and their number.

Results of sampling are located in the directory `presets`; results of the experiments are located in the directory `sampled_review` 

## Final analysis

Sanity check with experiments is located in [Sanity check notebook](./sanity_check.ipynb)

